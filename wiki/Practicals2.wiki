= Cleaning and processing the VMS and logbook data

== Introduction ==
Most often VMS data contains a lot of errors, many of them have a technical origin. Some are easy to detect, others a bit more tricky. The problems associated with logbooks are present as well, but most often these problems are caused by the fishers filling them out rather than by the processing and storing the data. 

Here we will provide some guidance on how to clean the VMS and logbook data. It needs to be set that these guidelines are not strict rules and that other rules are most likely more appropriate for your own dataset!

== Cleaning VMS data ===
Again, we start with loading / reading in the VMS data
<code>>data(tacsat)</code>
In VMS we can distinguish roughly two types of errors, 1- those errors that are without doubt incorrect and 2- errors for which a bit of thinking is required and might be part of a discussion.

Among the errors that are incorrect no matter what are VMS positions that are not associated with our globe, are true duplicates, have headings outside a compass range or are situated on land. Let's see if we can get rid of those first.
<code>>idx <- which(abs(tacsat$SI_LATI) > 90 | abs(tacsat$SI_LONG) > 180) #points not on the globe
>idx             <- unique(c(idx,which(tacsat$SI_HE < 0 | tacsat$SI_HE >  360))) #adding points with heading outside compass range
>length(idx)
</code>
Luckily, no problems as yet. On to points on land and duplicate records, which needs a bit more code.
<code>
>tacsat$SI_DATIM <- as.POSIXct(paste(tacsat$SI_DATE,  tacsat$SI_TIME,   sep=" "), tz="GMT", format="%d/%m/%Y  %H:%M") #create one date-time stamp
>uniqueTacsat    <- paste(tacsat$VE_REF,tacsat$SI_LATI,tacsat$SI_LONG,tacsat$SI_DATIM) #get records as a string to easily check for duplicates
>print(nrow(tacsat))
>tacsat          <- tacsat[!duplicated(uniqueTacsat),] #get rid of the duplicates
>print(nrow(tacsat))
</code>
And again, no problems, no duplicates (the only time you will see this most likely!). On to the points on land.
<code>
>data(europa) #a dataset already build into the package with the coastlines embedded
>head(europa)
>pols    <- lonLat2SpatialPolygons(lst=lapply(as.list(sort(unique(europa$SID))),
                function(x){data.frame(SI_LONG=subset(europa,SID==x)$X,
                                     SI_LATI=subset(europa,SID==x)$Y)}))
>idx     <- pointOnLand(tacsat,pols);
>table(idx)
>pol     <- tacsat[which(idx==1),] #points on land
>tacsat  <- tacsat[which(idx==0),] #tacsat not on land
</code>
Let's have a look where these points on land (according to our algorithm) are really located!
<code>
>library(maps);library(mapdata)
>map("worldHires",res=0,fill=T,col="darkgreen",xlim=c(-4,10),ylim=c(48,62)); map.axes()
>points(x=pol$SI_LONG,y=pol$SI_LATI,col="red",pch=19,cex=0.5)
</code>
As you can see, most of them are either on land, close to a harbour or on a river. Hence, it is important to use a detailed map to perform this calculation!

This means we are half-way cleaning the VMS dataset. The points left to do is to look into those 'errors' which are subject of discussion. They are VMS records associated with very high speeds, points in a harbour and 'pseudo-duplicates': points that are not exactly duplicates but can be considered as such. 
<code>
>hist(tacsat$SI_SP,breaks=100) #are there any outlying results?
>spThres <- 20 #I decide that 20knots is the maximum speed allowed in the dataset
>idx <- which(tacsat$SI_SP > spThres)
>tacsat <- tacsat[-idx,]
</code>
Only few points are associated with speeds higher than 20 knots. Let's move to the points in harbour. Obviously, these points are not true errors. Vessels are in harbour for large amounts of time. However, when analysing VMS we are hardly ever interested in these points and for that reason, we'd like to get rid of these. There are a large number of ports in Europe already and checking for all of them isn't easy, but we will give it a try! Again, to make things easy, a large port dataset is already available. Note however that this list is NOT complete and that you might need to add additional ports if you want to accurately perform this cleaning part.
<code>
>data(euharbours)
>idx <- pointInHarbour(tacsat$SI_LONG,tacsat$SI_LATI,harbours)
>pih <- tacsat[which(idx==1),]
>table(idx)
>tacsat <- tacsat[which(idx==0),]
</code>
All in all, 14787 points are located in harbour (that is about 15% of the total dataset!). Let's see where they really are located and if our analyses made sense.
<code>
>library(maps);library(mapdata)
>map("worldHires",res=0,fill=T,col="darkgreen",xlim=c(-4,10),ylim=c(48,62)); map.axes()
>points(x=pih$SI_LONG,y=pih$SI_LATI,col="red",pch=19,cex=0.5)
</code>
=== Exercise 1 ===
  # Note the dot-at-sea in the corner with the Netherlands, Germany and  Denmark. Have a look up Google Maps to which which harbour might be located there
  # How many harbours are stored in the build-in harbour dataset?
  # What does the 'range' mean in the harbour dataset?

What is left in cleaning the VMS dataset are the pseudo duplicates. What might happen is a malfunctioning of the VMS transmitter on-board vessels. They might transmit not on a regular interval of e.g. 1 or 2 hours but also within a few seconds or few minutes additional points. If you view these are true errors, you might want to remove these.
<code>
>tacsat <- sortTacsat(tacsat) #sort the data by vessel and time
>tacsatp<- intervalTacsat(tacsat,level="vessel",fill.na=T)
>hist(tacsatp$INTV,breaks=1000000,xlim=c(0,150)) #apparently there is a large number of records with a very small interval rate (close to zero)
>intThres <- 5 #I believe that interval rates lower than 5 minutes need to be removed
>tacsat          <- tacsatp[which(tacsatp$INTV > intThres),]
</code>

Note that this cleaning describes the absolute basic parts you need to screen for.

=== Exercise 2 ===
  # Think of two other aspects you need to screen for.
  # Check your data for these aspects and create appropriate graphs to show your result

== Cleaning logbook data ===
In cleaning the logbook data, similar issues as with cleaning the VMS data arise. Some errors are true mistakes and need to fixed / removed, other depend on the interpretation of the scientist. However, the origin of the error is most often a human mistake rather than a technical malfunction as we have seen with VMS data. For this reason it is advised to have a core knowledge of the fisheries data and how these sources are collected and stored.

The basic cleaning of logbook data consists of removing records where arrival date happens before departure date, duplicate records and easy to detect mistakes in the catch values recorded. Other aspects as mesh sizes and vessel lengths or horse power being outside a 'believable' range can be dealt with as well. 

<code>
>data(eflalo)
#In principle, the LE_ID identifier should be unique, this allows for an easy solution
>eflalo <- eflalo[!duplicated(eflalo$LE_ID),]
#If things are a bit more difficult
>ID <- paste(eflalo$VE_REF,eflalo$VE_FLT,eflalo$VE_COU,eflalo$VE_LEN,eflalo$VE_KW,eflalo$VE_KW,eflalo$VE_TON,eflalo$FT_REF,eflalo$FT_DCOU,eflalo$FT_DHAR,eflalo$FT_DDAT,eflalo$FT_DTIME,eflalo$FT_LCOU,eflalo$FT_LHAR,eflalo$FT_LDAT,eflalo$FT_LTIME)
>eflalo <- eflalo[!duplicated(ID),]
</code>
How about removing those records where arrival date occurs before departure date
<code>
>eflalop           <- eflalo
>eflalop$FT_DDATIM <- as.POSIXct(paste(eflalo$FT_DDAT,  eflalo$FT_DTIME,   sep=" "), tz="GMT", format="%d/%m/%Y  %H:%M") #turn date and time into one date-time stamp
>eflalop$FT_LDATIM <- as.POSIXct(paste(eflalo$FT_LDAT,  eflalo$FT_LTIME,   sep=" "), tz="GMT", format="%d/%m/%Y  %H:%M") #turn date and time into one date-time stamp
>idx               <- which(eflalop$FT_LDATIM >= eflalop$FT_DDATIM)
>print(nrow(eflalo))
>eflalo            <- eflalo[idx,] #only keep the records we want
>print(nrow(eflalo))
</code>
Because we carefully screened the example data before it was included in the package, you don't run into problems here.

Due to human mistakes, some catch records in the eflalo data end up being extremely large. Do we trust these values or not? Perhaps, an easy check is to see if these large catches are really big compared to other catches of the same species. If the second highest catch is more than 10 times as small as the biggest catch you can start to question the results. Again, knowledge on the fishery and their landings is required!
As a rule of thumb, I normally use a factor 30 difference as a quick check. 
<code>
#First get the species names in your eflalo dataset
>specs  <- substr(colnames(eflalo[grep("KG",colnames(eflalo))],7,9)
>lanThres <- 1.5 #this is approx 30x difference on a log10 scale)
>specBounds      <- lapply(as.list(specs),function(x){ idx   <- grep(x,colnames(eflalo))[grep("KG",colnames(eflalo)[grep(x,colnames(eflalo))])];
                                                        wgh   <- sort(unique(eflalo[which(eflalo[,idx]>0),idx]));
                                                        difw  <- diff(log10(wgh));
                                                        return(ifelse(any(difw > lanThres),wgh[rev(which(difw <= lanThres))],ifelse(length(wgh)==0,0,max(wgh,na.rm=T))))})
>specBounds      <- cbind(specs,unlist(specBounds)); >specBounds[which(is.na(specBounds[,2])==T),2] <- "0"

#Get the index of each of the species
>idx             <- unlist(lapply(as.list(specs),function(x){ idx   <- grep(x,colnames(eflalo))[grep("KG",colnames(eflalo)[grep(x,colnames(eflalo))])];
                                                        return(idx)}))
#If landing > bound -> NA
>for(iSpec in idx) eflalo[which(eflalo[,iSpec] > an(specBounds[(iSpec-idx[1]+1),2])),iSpec] <- NA

#Turn all other NA's in the eflalo dataset in KG and EURO columns to zero
>for(i in kgeur(colnames(eflalo))) eflalo[which(is.na(eflalo[,i]) == T),i] <- 0
